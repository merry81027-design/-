import streamlit as st
import pandas as pd

# ==========================================
# 1. ç¶²é åŸºæœ¬è¨­å®š
# ==========================================
st.set_page_config(page_title="é«˜ä¸­ç”Ÿè½é»åˆ†æç³»çµ±", page_icon="ğŸ“", layout="centered")
st.title("ğŸ“ 112-114å¹´ å¤§å­¸è½é»åˆ†æç³»çµ±")
st.markdown("ğŸ’¡ **æœå°‹æŠ€å·§**ï¼šæ”¯æ´è¤‡åˆé—œéµå­—ï¼Œä¾‹å¦‚è¼¸å…¥ `å°å¤§ è³‡å·¥` æˆ– `å¸«å¤§ å¿ƒè¼”`ã€‚è³‡æ–™èˆ‡é›²ç«¯è©¦ç®—è¡¨å³æ™‚åŒæ­¥ï¼")
st.divider()

# ==========================================
# 2. é€£çµ Google è©¦ç®—è¡¨ (å³æ™‚è®€å–)
# ==========================================
# ğŸ”´ è€å¸«è«‹æ³¨æ„ï¼šè«‹æŠŠä¸‹é¢çš„å¼•è™Ÿå…§å®¹ï¼Œæ›æˆæ‚¨å‰›å‰›è¤‡è£½çš„ã€Œè³‡æ–™åº« IDã€ï¼
SHEET_ID = "1VVm5MkdMzYF80dngcnHiBIWz7D1Sh0BnQeRvlKlA9DA" 
URL = f"https://docs.google.com/spreadsheets/d/{SHEET_ID}/export?format=csv"

# è¨­å®šå¿«å–æ™‚é–“ç‚º 10 ç§’ (ttl=10)ã€‚ä»£è¡¨æ‚¨åœ¨ Excel æ”¹å®Œè³‡æ–™ï¼Œç¶²é æœ€å¤š 10 ç§’å¾Œå°±æœƒæ›´æ–°ï¼
@st.cache_data(ttl=10)
def load_data(url):
    # ç›´æ¥è®€å– Google è©¦ç®—è¡¨åŒ¯å‡ºçš„ CSV æ ¼å¼
    df = pd.read_csv(url, dtype={'å­¸å¹´åº¦': str, 'ç³»æ ¡ä»£ç¢¼': str})
    df = df.dropna(subset=['ç³»æ ¡ä»£ç¢¼'])
    return df

try:
    with st.spinner("ğŸ”„ æ­£åœ¨èˆ‡é›²ç«¯è³‡æ–™åº«åŒæ­¥ä¸­..."):
        df = load_data(URL)
except Exception as e:
    st.error("âŒ ç„¡æ³•è®€å–è³‡æ–™åº«ï¼Œè«‹ç¢ºèª Google è©¦ç®—è¡¨æ¬Šé™æ˜¯å¦å·²è¨­å®šç‚ºã€ŒçŸ¥é“é€£çµçš„ä»»ä½•äººçš†å¯æª¢è¦–ã€ã€‚")
    st.stop()

# ==========================================
# 3. ç¶²é æœå°‹ä»‹é¢
# ==========================================
# å»ºç«‹ä¸€å€‹è¼¸å…¥æ¡†
user_input = st.text_input("ğŸ” è«‹è¼¸å…¥å­¸æ ¡æˆ–ç§‘ç³»é—œéµå­—ï¼š", placeholder="ä¾‹å¦‚ï¼šæ”¿å¤§ å¿ƒç†")

if user_input:
    # --- æœå°‹é‚è¼¯ ---
    normalized_input = user_input.replace('å°', 'è‡º')
    keywords = normalized_input.split()
    
    df['full_text'] = df['å­¸æ ¡åç¨±'].astype(str) + " " + df['æ ¡ç³»åç¨±'].astype(str)
    
    mask = pd.Series([True] * len(df))
    for k in keywords:
        mask = mask & df['full_text'].str.contains(k, na=False)
        
    candidates = df[mask]
    
    # å‚™ç”¨æœå°‹ (è™•ç†ç°¡é«”å°)
    if candidates.empty:
        raw_keywords = user_input.split()
        mask_retry = pd.Series([True] * len(df))
        for k in raw_keywords:
            mask_retry = mask_retry & df['full_text'].str.contains(k, na=False)
        candidates = df[mask_retry]

    # --- é¡¯ç¤ºçµæœ ---
    if candidates.empty:
        st.warning(f"âš ï¸ æ‰¾ä¸åˆ°åŒ…å«ã€Œ{user_input}ã€çš„ç§‘ç³»ï¼Œè«‹å˜—è©¦æ›´æ›æˆ–ç¸®çŸ­é—œéµå­—ã€‚")
    else:
        target_codes = candidates['ç³»æ ¡ä»£ç¢¼'].unique()
        st.success(f"ğŸ¯ æ‰¾åˆ° {len(target_codes)} å€‹ç›¸é—œç§‘ç³»ï¼")
        
        for code in target_codes:
            history_data = df[df['ç³»æ ¡ä»£ç¢¼'] == code]
            history_data = history_data.sort_values(by='å­¸å¹´åº¦', ascending=False)
            
            school_name = history_data.iloc[0]['å­¸æ ¡åç¨±']
            dept_names = history_data['æ ¡ç³»åç¨±'].unique()
            dept_name_display = " / ".join(dept_names)
            
            # ä½¿ç”¨ç¶²é çš„æ’ç‰ˆå…ƒä»¶
            st.subheader(f"ğŸ« ã€{school_name}ã€‘")
            st.caption(f"ğŸ“Œ ç³»åç´€éŒ„ï¼š{dept_name_display} (ä»£ç¢¼ï¼š{code})")
            
            cols = ['å­¸å¹´åº¦', 'æ ¡ç³»åç¨±', 'æ‹›ç”Ÿåé¡', 'ç¯©é¸ä¸€', 'ç¯©é¸äºŒ', 'ç¯©é¸ä¸‰', 'ç¯©é¸å››', 'ç¯©é¸äº”']
            show_cols = [c for c in cols if c in history_data.columns]
            
            # åœ¨ç¶²é ä¸Šç•«å‡ºæ¼‚äº®çš„è¡¨æ ¼ï¼Œhide_index=True å¯ä»¥éš±è—æœ€å‰é¢çš„æµæ°´è™Ÿ
            st.dataframe(history_data[show_cols], hide_index=True, use_container_width=True)
            st.divider()
